{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(-inf, inf, (8,), float32)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('LunarLander-v2')\n",
    "# env.observation_space: 垂直及水平座標、速度、角度、加速度等等\n",
    "# env.action_space: 0 代表不採取任何行動, 2 代表主引擎向下噴射, 1, 3 則是向左右噴射\n",
    "# initial_state = env.reset()\n",
    "# random_action = env.action_space.sample()\n",
    "# observation, reward, done, info = env.step(random_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "# img = plt.imshow(env.render(mode='rgb_array'))\n",
    "# env.render 就會有畫面\n",
    "total_reward = 0\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "#     img.set_data(env.render(mode='rgb_array'))\n",
    "#     display.display(plt.gcf()) # reutrn current figure\n",
    "#     display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(8, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 4)\n",
    "\n",
    "    def forward(self, state):\n",
    "        hid = torch.tanh(self.fc1(state))\n",
    "        hid = torch.tanh(self.fc2(hid))\n",
    "        return F.softmax(self.fc3(hid), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientAgent():\n",
    "\n",
    "    def __init__(self, network):\n",
    "        self.network = network\n",
    "        self.optimizer = optim.SGD(self.network.parameters(), lr=0.001)\n",
    "\n",
    "    def learn(self, log_probs, rewards):\n",
    "        loss = (-log_probs * rewards).sum()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def sample(self, state):\n",
    "        action_prob = self.network(torch.FloatTensor(state))\n",
    "        action_dist = Categorical(action_prob)\n",
    "        action = action_dist.sample()\n",
    "        log_prob = action_dist.log_prob(action)\n",
    "        return action.item(), log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.network.train()  # 訓練前，先確保 network 處在 training 模式\n",
    "EPISODE_PER_BATCH = 5  # 每蒐集 5 個 episodes 更新一次 agent\n",
    "NUM_BATCH = 400        # 總共更新 400 次\n",
    "\n",
    "avg_total_rewards, avg_final_rewards = [], []\n",
    "\n",
    "prg_bar = tqdm(range(NUM_BATCH))\n",
    "for batch in prg_bar:\n",
    "\n",
    "    log_probs, rewards = [], []\n",
    "    total_rewards, final_rewards = [], []\n",
    "\n",
    "    # 蒐集訓練資料\n",
    "    for episode in range(EPISODE_PER_BATCH):\n",
    "        \n",
    "        state = env.reset()\n",
    "        total_reward, total_step = 0, 0\n",
    "\n",
    "        while True:\n",
    "\n",
    "            action, log_prob = agent.sample(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            total_step += 1\n",
    "\n",
    "            if done:\n",
    "                final_rewards.append(reward)\n",
    "                total_rewards.append(total_reward)\n",
    "                rewards.append(np.full(total_step, total_reward))  # 設定同一個 episode 每個 action 的 reward 都是 total reward\n",
    "                break\n",
    "\n",
    "    # 紀錄訓練過程\n",
    "    avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
    "    avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
    "    avg_total_rewards.append(avg_total_reward)\n",
    "    avg_final_rewards.append(avg_final_reward)\n",
    "    prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n",
    "\n",
    "    # 更新網路\n",
    "    rewards = np.concatenate(rewards, axis=0)\n",
    "    rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)  # 將 reward 正規標準化\n",
    "    agent.learn(torch.stack(log_probs), torch.from_numpy(rewards))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
